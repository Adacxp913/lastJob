
相关知识总结：

A：简述 HDFS 的读写流程，要求不少于 300 字  
```
本答案参考《大数据技术原理与应用（第2版）3.6 HDFS的数据读写过程》
读：
1、通过FileSystem.open()方法打开文件，内部通过ClientProtocal.getBlockLocations()远程调用namenode，得到文件开始部分数据块的保存位置，并根据block和客户端的远近排序
2、调用FileSystem.read()读取文件，选择距离最近的block开始读，读取完毕后，关闭和该datanode的连接。
3、调用内部通过ClientProtocal.getBlockLocations()继续获取后续block的位置信息，继续读取
4、全部读取完毕后，掉用FileSystem.close关闭输入流结束读取
5、如果读取异常，则会尝试更换当前block的其他datanode来读取当前block。
   
写：
1、FileSystem.create()方法创建文件，内部通过DistributedFileSystem以rpc的方式调用namenode,新建1个文件。namenode会进行一些存在性、权限等检查，通过后创建1个输出流返回；
2、FileSystem.write()方法开始写文件。文件会被分为1个1个的小包存入1个内部队列。同时输出流会向namenode申请保存文件和副本block的datanode，并将这些datanode形成1个datanode管道；
3、1个个的小包会按顺序写入datanode管道，管道里的各个datanode收到数据包后，先写入自己的磁盘，然后把这个小包发给下游的datanode，下游datanode执行保存后再发给下游；
4、最后1个datanode写入成功后，会向它的前1个datanode反馈1个确认包，各个datanode按反的顺序往前面的datanode持续反馈确认包，直到回到第1个datanode，最后再回到输出流(客户端)；
5、所有文件包都写成功后，调用FileSystem.close()方法关闭输出流，结束文件的写出。
6、一般来说，1个block的多个副本的存放规则是：离客户端最近的节点存1份A，A不同机架的节点存1份B，A同1个机架的其他节点存1份C，如果有更多副本，就随机存储到更多节点上。

```
B：简述 Spark Shuffle 的工作原理，要求不少于 300 字
```
不同于mapreduce，每个环节都进行文件的落盘，导致shuffle的产生，spark创造了有向无环图的思路来优化数据处理流程。通过对处理过程进行分析，确定了宽依赖、窄依赖两种不同的数据转换形态。
窄依赖的两个转换之间，属于同1个jvm，因此其shuffle过程直接是内存处理，不存在文件写和读，因此效率大大的超过mapreduce。
宽依赖由于存在分布式跨节点数据交互，因此必然存在shuffle过程。又细分为shuffle write和shuffle read两个环节，分别进行文件的输出和文件的读取。
shuffle write时，会先将数据存在内存中，达到一定的阈值后再写到hdfs。实际会根据下游stage的情况确定文件数量，并对文件根据hash或排序规则进行分组，写到不同的文件中。
shuffle read时，会根据上游stage的情况，读取当前task需要的文件，一边读一边进行聚合处理。
```
C：简述 Flink SQL 的工作原理，要求不少于 300 字  
```
1、flink sql 同spark sql一样，在sql提交后，都会执行sql分析和优化过程，生成逻辑计划和物理计划，并最终通过codegen生成执行代码；
2、执行代码进一步编译，变成JobGraph提交运行,jobGraph中将窄依赖的task合并为1个stage，将宽依赖的task转换为独立的stage；
3、flink继续优化，根据数据流(文件)的分区（文件数）情况，生成对应的并行度，同1个task的多个并行度对应多个分区概念，不同stage之间的多个分区数据会存在shuffle的过程；
4、如果是批量数据sql处理，则类似于spark执行，最终计算完毕后数据进行输出或者写到输出表(sink)中；
5、如果是流式数据sql处理，则必须有窗口机制，将每个窗口内的数据进行聚合计算，然后输出到表(sink)中；
```


个人总结：
自开课到结课，已过半年之久。再次毕业之际，有很多感触，也简单的说一下几点。
1、首先感谢遇到班班，耐心的指导，温柔的催促我们学习学习再学习，虽然很唠叨，但是我们知道这些都是为了让我们能够按时学习，不浪费交的学费。虽然很唠叨，但是吧，我也很感谢遇到老师。
2、其次感谢金老师，张老师和两位助教老师。金老师的知识很渊博，人也很好玩，这点在上海疫情很严重的时候，教学只能直播方式进行时展现的淋漓尽致。老师总是很乐观，也很温柔的解答同学们的问题，
希望以后能有这种学术性的老师；接下来就是张老师，几乎每周六风雨不断的给我们上领教直播课，很腼腆的一个老师，知识也是没话说，反正比我厉害的都是没话说，很感谢老师每周六晚带领我们总结、拓展、
分析那抓头的作业问题；最后就是两位助教老师，我从最开始会有一些基础的莫名的问题，然后比较傻的在群里各种@两位老师，那个昵称“我”的老师总是会及时出现，帮我看问题，分析问题。总是在分析着分析着，
诶，我搞定了的状态中度过，估计老师也很抓马吧。
3、最后，感谢我自己，能够从最开始的一腔热血到后面的抓马，抓耳挠腮，想哭，死磕的状态中一直坚持着，即使听不懂了，也还在坚持，嗯，很感谢一直在坚持的自己。
开课之时，展望半年后，觉得无比遥远；结课之时，回首半年前，觉得稍纵即逝。这半年时间，对于学习大数据来说远远不够，还需要在日后不断的深钻琢磨，知识的海洋浩瀚无边，我也希望
自己能够一直坚持学习，一直做一个永远充满斗志和热情的人，享受在知识的海洋中被淹没的感觉。等到耄耋之年，也依旧充满斗志和热情！爱学习爱生活爱这个美好的世界！
最后，也祝各位老师和同学以我自己前程似锦，生活美满！
